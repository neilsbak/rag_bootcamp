{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d86f6cd",
   "metadata": {},
   "source": [
    "# Cohere Document Search with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168e6b6",
   "metadata": {},
   "source": [
    "This example shows how to use the Python [LlamaIndex](https://docs.llamaindex.ai/en/stable/) library to run a text-generation request against [Cohere's](https://cohere.com/) API, then augment that request using the text stored in a collection of local PDF documents.\n",
    "\n",
    "**Requirements:**\n",
    "- You will need an access key to Cohere's API key, which you can sign up for at (https://dashboard.cohere.com/welcome/login). A free trial account will suffice, but will be limited to a small number of requests.\n",
    "- After obtaining this key, store it in plain text in your home in directory in the `~/.cohere.key` file.\n",
    "- (Optional) Upload some pdf files into the `source_documents` subfolder under this notebook. We have already provided some sample pdfs, but feel free to replace these with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4da1f",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f637730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core import ServiceContext, SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "import chromadb\n",
    "from chromadb import Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf9ac",
   "metadata": {},
   "source": [
    "Set up some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edd103",
   "metadata": {},
   "source": [
    "Make sure other necessary items are in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b61e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    "#    os.environ[\"COHERE_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "#    os.environ[\"CO_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "#except Exception:\n",
    "#    print(f\"ERROR: You must have a Cohere API key available in your home directory at ~/.cohere.key\")\n",
    "\n",
    "# Look for the source-materials folder and make sure there is at least 1 pdf file here\n",
    "contains_pdf = False\n",
    "directory_path = \"./source_documents\"\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"ERROR: The {directory_path} subfolder must exist under this notebook\")\n",
    "for filename in os.listdir(directory_path):\n",
    "    contains_pdf = True if \".pdf\" in filename else contains_pdf\n",
    "if not contains_pdf:\n",
    "    print(f\"ERROR: The {directory_path} subfolder must contain at least one .pdf file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a22c5",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00061d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"])\n",
    "llm = Ollama(model=\"llama3\", request_timeout=30.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1c200",
   "metadata": {},
   "source": [
    "Without additional information, Cohere is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022.** Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the `source_documents` folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255ea68",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from source-materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d0304",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from `source_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5710c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source materials: 116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the pdfs\n",
    "pdf_folder_path = \"./source_documents\"\n",
    "documents = SimpleDirectoryReader(input_files=[f\"{pdf_folder_path}/Vanguard_ETF_Statutory_Prospectus_Single_VOE.pdf\"]).load_data()\n",
    "print(f\"Number of source materials: {len(documents)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7545e",
   "metadata": {},
   "source": [
    "## Define an embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc16fe",
   "metadata": {},
   "source": [
    "This embeddings model will convert the textual data from our PDF files into vector embeddings. These vector embeddings will later enable us to quickly find the chunk of text that most closely corresponds to our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1048c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/cfgm_qv96_z8s4pnr8frqh6h0000gn/T/ipykernel_98592/2067548003.py:12: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "#embed_model = CohereEmbedding(\n",
    "#    model_name=\"embed-english-v3.0\",\n",
    "#    input_type=\"search_query\"\n",
    "#)\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model,\n",
    "    llm=llm,\n",
    "    chunk_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1690e",
   "metadata": {},
   "source": [
    "## Storage: Store the documents in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "075ede5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 116/116 [00:04<00:00, 27.46it/s]\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=True)\n",
    "index = VectorStoreIndex(documents, service_context=service_context, storage_context=storage_context, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24c02b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_json = json.dumps(chroma_collection.get())\n",
    "data = json.loads(data_json)\n",
    "chroma_client.delete_collection(\"newcollection\")\n",
    "collection = chroma_client.create_collection(\"newcollection\")\n",
    "collection.add(ids=data[\"ids\"], embeddings=data[\"embeddings\"], documents=data[\"documents\"], metadatas=data[\"metadatas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008507b",
   "metadata": {},
   "source": [
    "## Retrieval: Now do a search to retrieve the chunk of document text that most closely matches our original query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-douglas",
   "metadata": {},
   "source": [
    "## Setup retriever and reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "neural-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_query_retriever = index.as_retriever(service_context=service_context)\n",
    "#reranker = CohereRerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5f36798",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected to load a single index, but got 3 instead. Please specify index_id.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mstorage_context\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m----> 2\u001b[0m updated_index \u001b[38;5;241m=\u001b[39m \u001b[43mload_index_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStorageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python/rag_bootcamp/TD_team_B/venv/lib/python3.11/site-packages/llama_index/core/indices/loading.py:40\u001b[0m, in \u001b[0;36mload_index_from_storage\u001b[0;34m(storage_context, index_id, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo index in storage context, check if you specified the right persist_dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected to load a single index, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify index_id.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Expected to load a single index, but got 3 instead. Please specify index_id."
     ]
    }
   ],
   "source": [
    "# dict = index.storage_context.to_dict()\n",
    "index.storage_context.persist(\"./test_index\")\n",
    "storage_context = StorageContext.from_defaults()\n",
    "updated_index = load_index_from_storage(StorageContext.from_dict(dict), service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d420d81",
   "metadata": {},
   "source": [
    "## Query Response pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "registered-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_to_query(query):\n",
    "    # search_query_retrieved_nodes = search_query_retriever.retrieve(query)\n",
    "    # print(f\"Search query retriever found {len(search_query_retrieved_nodes)} results\")\n",
    "    # print(f\"First result example:\\n{search_query_retrieved_nodes[0]}\\n\")\n",
    "    query_engine = index.as_query_engine(\n",
    "        # streaming=True\n",
    "        #node_postprocessors = [reranker]\n",
    "    )\n",
    "    result = query_engine.query(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29366790",
   "metadata": {},
   "source": [
    "## Get the Fund Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc1d785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fund Name: Vanguard Extended Market Index Fund\n"
     ]
    }
   ],
   "source": [
    "fund_name = get_response_to_query(\"What is the name of the fund? Give only the name without additional comments. The name of the fund is: \")\n",
    "print(f\"Fund Name: {fund_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12bf2e",
   "metadata": {},
   "source": [
    "## Get responses to key queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "960dd030",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is the investment strategy of the fund?\",\n",
    "    \"What are the investment objectives of the fund?\",\n",
    "    \"Who are the key people in the management team?\",\n",
    "    \"What is the investment philosphy of the fund regarding ESG (Environmental, Social, and Governance)?\",\n",
    "    \"What industries, markets, or types of securities is the fund want exposure to?\",\n",
    "    \"What investment tools (derivatives, leverage, etc) does does the fund use to achieve their investment goals?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "competent-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for query in queries:\n",
    "    result = get_response_to_query(query)\n",
    "    responses.append(result.response)\n",
    "\n",
    "response_answer_pairs = zip(queries, responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "competitive-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the investment strategy of the fund?\n",
      "Empty Response\n",
      "\n",
      "What are the investment objectives of the fund?\n",
      "Empty Response\n",
      "\n",
      "Who are the key people in the management team?\n",
      "Empty Response\n",
      "\n",
      "What is the investment philosphy of the fund regarding ESG (Environmental, Social, and Governance)?\n",
      "Empty Response\n",
      "\n",
      "What industries, markets, or types of securities is the fund want exposure to?\n",
      "Empty Response\n",
      "\n",
      "What investment tools (derivatives, leverage, etc) does does the fund use to achieve their investment goals?\n",
      "Empty Response\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_answer_text = \"\"\n",
    "for (query, response) in response_answer_pairs:\n",
    "    response_answer_text = f\"{response_answer_text}{query}\\n{response}\\n\\n\"\n",
    "\n",
    "print(response_answer_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c651907",
   "metadata": {},
   "source": [
    "## Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3c128ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        f\"You are an expert Mutual Fund analyst for a bank, and you privide answers to your boss about whether the bank should purchase the fund named {fund_name}.\"\n",
    "        f\"  You have answered these key questions about the fund:\\n {response_answer_text}\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d77dc1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the prospectus, the level of risk for the Vanguard Total Stock Market Index Fund appears to be moderate to high. The fund invests at least 80% of its assets in stocks that make up its target index, which means it is exposed to the risks associated with stock market investments. The prospectus also highlights several key risks, including:\n",
      "\n",
      "1. Stock market risk: The chance that stock prices overall will decline. Stock markets tend to move in cycles, with periods of rising prices and periods of falling prices.\n",
      "2. Index sampling risk: The chance that the securities selected for the fund, in the aggregate, will not provide investment performance matching that of the fund's target index. This risk is expected to be low.\n",
      "3. Derivatives risk: The use of derivatives, such as equity futures and total return swaps, can amplify gains or losses depending on market conditions. While these tools can help reduce transaction costs and stay fully invested, they also introduce additional risks.\n",
      "4. Leverage risk: The fund may use leverage to increase its potential returns, but this can also amplify losses.\n",
      "\n",
      "Overall, the fund's investment strategy is designed to provide broad exposure to the U.S. stock market while minimizing transaction costs and seeking to track its target index. However, as with any investment, there is always a risk of loss, and investors should carefully consider their risk tolerance before investing in the fund.\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat_engine.chat(\"What is the level of risk for the fund?\")\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "462d115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the prospectus, the level of risk for the Vanguard Total Stock Market Index Fund is likely to be higher than most other mutual funds. This is because the fund invests at least 80% of its assets in stocks that make up its target index, which means it is exposed to the risks associated with stock market investments. Additionally, the prospectus highlights several key risks, including stock market risk, index sampling risk, derivatives risk, and leverage risk, which can all contribute to a higher level of risk compared to other funds that may have more limited exposure to these risks.\n",
      "\n",
      "It's important to note that the risk level of a mutual fund can vary depending on its investment strategy and the specific securities it holds. Some funds may be more conservative and focused on income generation, while others may take on more risk in pursuit of higher returns. It's always important to carefully evaluate the investment objectives and strategies of any mutual fund before investing, and to consider your own risk tolerance and financial goals when making investment decisions.\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat_engine.chat(\"is it higher or lower than most funds?\")\n",
    "print(chat_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
