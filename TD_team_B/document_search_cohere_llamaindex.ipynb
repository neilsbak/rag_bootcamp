{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d86f6cd",
   "metadata": {},
   "source": [
    "# Cohere Document Search with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168e6b6",
   "metadata": {},
   "source": [
    "This example shows how to use the Python [LlamaIndex](https://docs.llamaindex.ai/en/stable/) library to run a text-generation request against [Cohere's](https://cohere.com/) API, then augment that request using the text stored in a collection of local PDF documents.\n",
    "\n",
    "**Requirements:**\n",
    "- You will need an access key to Cohere's API key, which you can sign up for at (https://dashboard.cohere.com/welcome/login). A free trial account will suffice, but will be limited to a small number of requests.\n",
    "- After obtaining this key, store it in plain text in your home in directory in the `~/.cohere.key` file.\n",
    "- (Optional) Upload some pdf files into the `source_documents` subfolder under this notebook. We have already provided some sample pdfs, but feel free to replace these with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4da1f",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f637730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "import chromadb\n",
    "from chromadb import Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf9ac",
   "metadata": {},
   "source": [
    "Set up some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dd4e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edd103",
   "metadata": {},
   "source": [
    "Make sure other necessary items are in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74b61e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    "#    os.environ[\"COHERE_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "#    os.environ[\"CO_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "#except Exception:\n",
    "#    print(f\"ERROR: You must have a Cohere API key available in your home directory at ~/.cohere.key\")\n",
    "\n",
    "# Look for the source-materials folder and make sure there is at least 1 pdf file here\n",
    "contains_pdf = False\n",
    "directory_path = \"./source_documents\"\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"ERROR: The {directory_path} subfolder must exist under this notebook\")\n",
    "for filename in os.listdir(directory_path):\n",
    "    contains_pdf = True if \".pdf\" in filename else contains_pdf\n",
    "if not contains_pdf:\n",
    "    print(f\"ERROR: The {directory_path} subfolder must contain at least one .pdf file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a22c5",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "00061d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"])\n",
    "llm = Ollama(model=\"llama3\", request_timeout=30.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1c200",
   "metadata": {},
   "source": [
    "Without additional information, Cohere is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022.** Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the `source_documents` folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255ea68",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from source-materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d0304",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from `source_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5710c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source materials: 116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the pdfs\n",
    "pdf_folder_path = \"./source_documents\"\n",
    "documents = SimpleDirectoryReader(input_files=[f\"{pdf_folder_path}/Vanguard_ETF_Statutory_Prospectus_Single_VOE.pdf\"]).load_data()\n",
    "print(f\"Number of source materials: {len(documents)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7545e",
   "metadata": {},
   "source": [
    "## Define an embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc16fe",
   "metadata": {},
   "source": [
    "This embeddings model will convert the textual data from our PDF files into vector embeddings. These vector embeddings will later enable us to quickly find the chunk of text that most closely corresponds to our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1048c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed_model = CohereEmbedding(\n",
    "#    model_name=\"embed-english-v3.0\",\n",
    "#    input_type=\"search_query\"\n",
    "#)\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ce55e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "db.delete_collection(\"test\");\n",
    "chroma_collection = db.get_or_create_collection(\"test\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=embed_model)\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm\n",
    "    #node_postprocessors = [reranker]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1690e",
   "metadata": {},
   "source": [
    "## Storage: Store the documents in a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008507b",
   "metadata": {},
   "source": [
    "## Retrieval: Now do a search to retrieve the chunk of document text that most closely matches our original query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-douglas",
   "metadata": {},
   "source": [
    "## Setup retriever and reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "neural-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_query_retriever = index.as_retriever(llm=llm)\n",
    "#reranker = CohereRerank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d420d81",
   "metadata": {},
   "source": [
    "## Query Response pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "registered-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_to_query(query):\n",
    "    # search_query_retrieved_nodes = search_query_retriever.retrieve(query)\n",
    "    # print(f\"Search query retriever found {len(search_query_retrieved_nodes)} results\")\n",
    "    # print(f\"First result example:\\n{search_query_retrieved_nodes[0]}\\n\")\n",
    "    query_engine = index.as_query_engine(\n",
    "        llm=llm\n",
    "        # streaming=True\n",
    "        #node_postprocessors = [reranker]\n",
    "    )\n",
    "    result = query_engine.query(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29366790",
   "metadata": {},
   "source": [
    "## Get the Fund Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc1d785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fund Name: Vanguard ETF.\n"
     ]
    }
   ],
   "source": [
    "fund_name = get_response_to_query(\"What is the name of the fund? Give only the name without additional comments. The name of the fund is: \")\n",
    "print(f\"Fund Name: {fund_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12bf2e",
   "metadata": {},
   "source": [
    "## Get responses to key queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "960dd030",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is the investment strategy of the fund?\",\n",
    "    \"What are the investment objectives of the fund?\",\n",
    "    \"Who are the key people in the management team?\",\n",
    "    \"What is the investment philosphy of the fund regarding ESG (Environmental, Social, and Governance)?\",\n",
    "    \"What industries, markets, or types of securities is the fund want exposure to?\",\n",
    "    \"What investment tools (derivatives, leverage, etc) does does the fund use to achieve their investment goals?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "competent-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for query in queries:\n",
    "    result = get_response_to_query(query)\n",
    "    responses.append(result.response)\n",
    "\n",
    "response_answer_pairs = zip(queries, responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "competitive-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the investment strategy of the fund?\n",
      "Under normal circumstances, each Fund will invest at least 80% of its assets in the stocks that make up its target index. A Fund may change its 80% policy only upon 60 days' notice to shareholders.\n",
      "\n",
      "What are the investment objectives of the fund?\n",
      "The Vanguard Value ETF seeks to track the performance of a benchmark index that measures the investment return of large-capitalization value stocks.\n",
      "\n",
      "Who are the key people in the management team?\n",
      "The key people in the management team include Walter Nejman, Portfolio Manager at Vanguard, who has co-managed the Fund since 2016; Gerard C. O'Reilly, Principal of Vanguard, who has managed the Fund since 1994 (co-managed since 2016); and Aaron Choi, CFA, Kenny Narzikul, CFA, and Nicholas Birkett, CFA, Portfolio Managers at Vanguard, who have co-managed the Fund since August 2023.\n",
      "\n",
      "What is the investment philosphy of the fund regarding ESG (Environmental, Social, and Governance)?\n",
      "The provided context does not mention ESG (Environmental, Social, and Governance) or any related concepts such as environmental risk or social responsibility. The focus appears to be on traditional market risks, investment style risk, index replicating risk, and ETF-specific risks. Therefore, it is impossible to determine the fund's investment philosophy regarding ESG from this context.\n",
      "\n",
      "What industries, markets, or types of securities is the fund want exposure to?\n",
      "The fund's target index tracks a subset of the U.S. stock market, which could cause the fund to perform differently from the overall stock market. This implies that the fund wants exposure to the U.S. stock market, specifically mid-capitalization value stocks.\n",
      "\n",
      "What investment tools (derivatives, leverage, etc) does does the fund use to achieve their investment goals?\n",
      "According to the provided context, the Funds will not use derivatives for speculation or leveraging (magnifying) investment returns. Additionally, each Fund's daily cash balance may be invested in Vanguard Market Liquidity Fund and/or Vanguard Municipal Cash Management Fund (CMT Funds), which are low-cost money market funds.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_answer_text = \"\"\n",
    "for (query, response) in response_answer_pairs:\n",
    "    response_answer_text = f\"{response_answer_text}{query}\\n{response}\\n\\n\"\n",
    "\n",
    "print(response_answer_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c651907",
   "metadata": {},
   "source": [
    "## Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e3c128ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    llm=llm,\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        f\"You are an expert Mutual Fund analyst for a bank, and you privide answers to your boss about whether the bank should purchase the fund named {fund_name}.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d77dc1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the statutory prospectus, I would say that the level of risk for the Vanguard ETF (VOE) is moderate to high. The fund invests in mid-capitalization value stocks, which are known for their volatility and potential to trail returns from the overall stock market.\n",
      "\n",
      "The prospectus highlights several risks associated with the fund:\n",
      "\n",
      "1. Investment style risk: Mid-cap stocks tend to be more volatile than large-cap stocks, and they often perform differently.\n",
      "2. Index replicating risk: The fund may not be able to replicate its target index exactly, which could result in differences between the fund's performance and the underlying market.\n",
      "\n",
      "Additionally, as an exchange-traded fund (ETF), VOE is subject to additional risks, such as:\n",
      "\n",
      "1. Market price vs. NAV: The market price of ETF Shares may differ significantly from their net asset value (NAV).\n",
      "2. Trading halts: Trading in ETF Shares may be halted due to individual or marketwide trading halts.\n",
      "3. Delisting: ETF Shares may be delisted without prior notice.\n",
      "\n",
      "Overall, I would rate the risk level for VOE as moderate to high, reflecting its focus on mid-cap value stocks and the potential for increased volatility. Investors should carefully consider their investment goals and risk tolerance before investing in this fund.\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat_engine.chat(\"What is the level of risk for the fund?\")\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "462d115c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "property 'chat_history' of 'ContextChatEngine' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m chat_engine2 \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_chat_engine(\n\u001b[1;32m      2\u001b[0m     llm\u001b[38;5;241m=\u001b[39mOllama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblah\u001b[39m\u001b[38;5;124m\"\u001b[39m, request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30.0\u001b[39m),\n\u001b[1;32m      3\u001b[0m     chat_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     ),    \n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mchat_engine2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_history\u001b[49m \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m chat_engine2\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis it higher or lower than most funds?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat_response)\n",
      "\u001b[0;31mAttributeError\u001b[0m: property 'chat_history' of 'ContextChatEngine' object has no setter"
     ]
    }
   ],
   "source": [
    "chat_engine2 = index.as_chat_engine(\n",
    "    llm=Ollama(model=\"blah\", request_timeout=30.0),\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    chat_history = chat_engine.chat_history,\n",
    "    system_prompt=(\n",
    "        f\"You are an expert Mutual Fund analyst for a bank, and you privide answers to your boss about whether the bank should purchase the fund named {fund_name}.\"\n",
    "    ),    \n",
    ")\n",
    "chat_response = chat_engine2.chat(\"is it higher or lower than most funds?\")\n",
    "print(chat_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
